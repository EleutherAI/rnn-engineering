{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/gpaulo/miniconda3/envs/new_transformers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight.models.Mamba import MambaInterp\n",
    "from nnsight import LanguageModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import MambaForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n",
    "mamba_sight = LanguageModel(\"state-spaces/mamba-2.8b-hf\",tokenizer=tokenizer)\n",
    "mamba = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: cat, dog: dog, tree:tree, window: window,\n",
      "        door: door, lightbulb: lightbulb, mouse: mouse, elephant: elephant,\n",
      "        turtle: turtle, catfish: catfish, monkey: monkey, pig: pig,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"cat: cat, dog: dog, tree:tree, window:\"\n",
    "input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "output = mamba.generate(input_ids, max_length=60, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: dog, tree:window, pyramid:sand, cat: dog, and so on.\n",
      "\n",
      "In this paper, we extend the classic\n"
     ]
    }
   ],
   "source": [
    "input = \"cat: dog, tree:window, pyramid:sand, cat:\"\n",
    "input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "output = mamba.generate(input_ids, max_length=30, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absfads: absfads, daksfjksadshajsd: daksfjksadshajsd, tffassdwe:tffassdwe, tffassdwe:tffassdwe\n",
      "<kennyloggins> i\n"
     ]
    }
   ],
   "source": [
    "input = \"absfads: absfads, daksfjksadshajsd: daksfjksadshajsd, tffassdwe:tffassdwe, tffassdwe:\"\n",
    "input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "output = mamba.generate(input_ids, max_length=60, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: absfads, 2: daksfjksadshajsd, 3:tffassdwe, 1:sdsaafsad, 3:adfkjskd, 1:sadsfadshj, 2:sjkaskdksad\n"
     ]
    }
   ],
   "source": [
    "input = \"1: absfads, 2: daksfjksadshajsd, 3:tffassdwe, 1:\"\n",
    "input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "output = mamba.generate(input_ids, max_length=60, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(104, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x MambaBlock(\n",
       "        (norm): MambaRMSNorm()\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (x_proj): Linear(in_features=1024, out_features=112, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=104, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSM(discrete_A, deltaB_u, C, seq_len):\n",
    "    hidden_dimension = 2560\n",
    "    state_size = 16\n",
    "    scan_outputs = []\n",
    "    ssm_state = torch.zeros((1, hidden_dimension*2, state_size))\n",
    "    ssm_states = []\n",
    "    for i in range(seq_len):\n",
    "        ssm_state = discrete_A[:, :, i, :] * ssm_state + deltaB_u[:, :, i, :]      # [batch, intermediade_size, ssm_state]\n",
    "        scan_output = torch.matmul(ssm_state, C[:, i, :].unsqueeze(-1))  # [batch, intermediade_size, 1]\n",
    "        scan_outputs.append(scan_output[:, :, 0])\n",
    "        ssm_states.append(ssm_state.save())\n",
    "    return torch.stack(scan_outputs, dim=-1),ssm_states   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Ablating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the name of any given person could be changed and nobody knew that the name had been\n"
     ]
    }
   ],
   "source": [
    "state_size = 16\n",
    "dt_size = 160\n",
    "seq_len = 2\n",
    "with mamba_sight.generate(\"Once upon\",max_new_tokens=20, do_sample=True, top_k=50, top_p=0.95) as tracer:\n",
    "    seq_len = 2\n",
    "    ssm_parameters = mamba_sight.backbone.layers[0].mixer.x_proj.output\n",
    "    projected_states = mamba_sight.backbone.layers[0].mixer.in_proj.output.transpose(1, 2)\n",
    "    projected_states.save()\n",
    "    a , gate = projected_states.chunk(2, dim=1)\n",
    "    \n",
    "    timestep,B,C = torch.split(ssm_parameters,[dt_size,state_size,state_size],dim=-1)\n",
    "    discrete_time_step = mamba_sight.backbone.layers[0].mixer.dt_proj.output                            \n",
    "    discrete_time_step = nn.functional.softplus(discrete_time_step).transpose(1, 2) \n",
    "\n",
    "    hidden_states = mamba_sight.backbone.layers[0].mixer.act(mamba_sight.backbone.layers[0].mixer.conv1d.output[..., :seq_len])\n",
    "    A = -torch.exp(mamba_sight.backbone.layers[0].mixer.A_log)\n",
    "    discrete_A = torch.exp(A[None, :, None, :] * discrete_time_step[:, :, :, None]) # [batch, intermediate_size, seq_len, ssm_state_size]\n",
    "    discrete_B = discrete_time_step[:, :, :, None] * B[:, None, :, :].float()       # [batch, intermediade_size, seq_len, ssm_state_size]\n",
    "    deltaB_u = discrete_B * hidden_states[:, :, :, None].float()\n",
    "    ssm,ssm_state = SSM(discrete_A, deltaB_u, C, seq_len)\n",
    "    ssm = ssm+hidden_states*mamba_sight.backbone.layers[0].mixer.D[None, :, None]\n",
    "    ssm.save()\n",
    "    gated_ssm = ssm*mamba_sight.backbone.layers[0].mixer.act(gate)\n",
    "    gated_ssm.save()\n",
    "    input = mamba_sight.backbone.layers[0].mixer.out_proj.input[0][0].save()\n",
    "    output = mamba_sight.generator.output.save()    \n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "#print(more.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_input = input\n",
    "original_gated = gated_ssm\n",
    "original_ssm = ssm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation 1: Unitary SSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl who was very, very, very, very, very,\n"
     ]
    }
   ],
   "source": [
    "state_size = 16\n",
    "dt_size = 160\n",
    "with torch.no_grad():\n",
    "    with mamba_sight.generate(\"Once upon\",max_new_tokens=20) as tracer:\n",
    "        seq_len = mamba_sight.backbone.layers[0].mixer.in_proj.output.shape[1]\n",
    "        \n",
    "        \n",
    "        #ssm=1\n",
    "        for i in range(29,30):\n",
    "            ssm_parameters = mamba_sight.backbone.layers[0].mixer.x_proj.output\n",
    "            projected_states = mamba_sight.backbone.layers[0].mixer.in_proj.output.transpose(1, 2)\n",
    "            projected_states.save()\n",
    "            a , gate = projected_states.chunk(2, dim=1)\n",
    "            \n",
    "            timestep,B,C = torch.split(ssm_parameters,[dt_size,state_size,state_size],dim=-1)\n",
    "            discrete_time_step = mamba_sight.backbone.layers[0].mixer.dt_proj.output                            \n",
    "            discrete_time_step = nn.functional.softplus(discrete_time_step).transpose(1, 2) \n",
    "\n",
    "            hidden_states = mamba_sight.backbone.layers[0].mixer.act(mamba_sight.backbone.layers[0].mixer.conv1d.output[..., :seq_len])\n",
    "            A = -torch.exp(mamba_sight.backbone.layers[0].mixer.A_log)\n",
    "            discrete_A = torch.exp(A[None, :, None, :] * discrete_time_step[:, :, :, None]) # [batch, intermediate_size, seq_len, ssm_state_size]\n",
    "            discrete_B = discrete_time_step[:, :, :, None] * B[:, None, :, :].float()       # [batch, intermediade_size, seq_len, ssm_state_size]\n",
    "            deltaB_u = discrete_B * hidden_states[:, :, :, None].float()\n",
    "            ssm,ssm_state = SSM(discrete_A, deltaB_u, C, seq_len)\n",
    "            ssm = ssm+hidden_states*mamba_sight.backbone.layers[0].mixer.D[None, :, None]\n",
    "            \n",
    "            gated_ssm = ssm*mamba_sight.backbone.layers[i].mixer.act(gate)\n",
    "            gated_ssm.save()\n",
    "            gated_ssm = gated_ssm.transpose(1, 2)\n",
    "            mamba_sight.backbone.layers[i].mixer.out_proj.input[0][0][:] = gated_ssm[0]\n",
    "            input = mamba_sight.backbone.layers[i].mixer.out_proj.input[0][0]\n",
    "        input.save()\n",
    "        output = mamba_sight.generator.output.save()\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "#print(more.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2715, -0.2718,  0.1379,  ..., -0.1511, -0.0687, -0.2392],\n",
       "        [-0.1506, -0.1783, -0.2216,  ...,  0.1278, -0.2739,  0.1495],\n",
       "        [ 0.2125,  1.5953,  0.1440,  ..., -0.2545,  0.5127,  0.2982],\n",
       "        ...,\n",
       "        [-0.2784, -0.2781, -0.1417,  ..., -0.2575, -0.2784, -0.2480],\n",
       "        [ 0.2984, -0.1127, -0.2026,  ..., -0.0838,  0.0210,  0.1993],\n",
       "        [-0.2772, -0.2221, -0.2215,  ..., -0.2477, -0.2764, -0.2683]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.8533e-03, -6.0776e-04, -2.9002e-04,  ..., -2.7613e-04,\n",
       "          -9.6215e-04, -6.7582e-05],\n",
       "         [ 2.8024e-03,  5.5461e-04, -4.3667e-03,  ...,  2.7560e-04,\n",
       "          -2.1598e-03,  1.6535e-04],\n",
       "         [-4.1623e-03, -5.8760e-03, -7.8286e-03,  ..., -4.2756e-02,\n",
       "          -4.6123e-03,  1.5350e-03],\n",
       "         ...,\n",
       "         [ 2.7521e-03, -2.8168e-03,  1.1099e-03,  ...,  2.8431e-04,\n",
       "          -1.6862e-03,  1.8183e-04],\n",
       "         [-8.8093e-03,  3.3298e-04, -5.7871e-03,  ...,  4.7501e-04,\n",
       "           6.1060e-04, -3.2732e-04],\n",
       "         [ 3.1671e-03, -2.1554e-03, -4.9180e-04,  ...,  2.3122e-03,\n",
       "          -5.3262e-03,  1.7630e-04]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablate all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower is in the city of of the city.\n",
      "\n",
      "The city is also\n"
     ]
    }
   ],
   "source": [
    "state_size = 16\n",
    "dt_size = 160\n",
    "with mamba_sight.generate(\"The Eiffel Tower is in the city of\",max_new_tokens=10) as tracer:\n",
    "    seq_len = mamba_sight.backbone.layers[0].mixer.in_proj.output.shape[1]\n",
    "    \n",
    "    ssm_parameters = mamba_sight.backbone.layers[0].mixer.x_proj.output\n",
    "    projected_states = mamba_sight.backbone.layers[0].mixer.in_proj.output.transpose(1, 2)\n",
    "    projected_states.save()\n",
    "    a , gate = projected_states.chunk(2, dim=1)\n",
    "    \n",
    "    ssm=0\n",
    "    gated_ssm = ssm*mamba_sight.backbone.layers[0].mixer.act(gate)\n",
    "    gated_ssm.save()\n",
    "    #print(gated_ssm.shape)\n",
    "    gated_ssm = gated_ssm.transpose(1, 2)\n",
    "    for i in range(64):\n",
    "        mamba_sight.backbone.layers[i].mixer.out_proj.input[0][0][:] = gated_ssm[0]\n",
    "    input = mamba_sight.backbone.layers[63].mixer.out_proj.input[0][0].save()\n",
    "    output = mamba_sight.generator.output.save()\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "#print(more.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0., -0., 0.,  ..., -0., -0., -0.],\n",
       "         [-0., -0., -0.,  ..., 0., -0., 0.],\n",
       "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
       "         ...,\n",
       "         [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "         [0., -0., -0.,  ..., -0., 0., 0.],\n",
       "         [-0., -0., -0.,  ..., -0., -0., -0.]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
