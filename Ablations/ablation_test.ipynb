{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/gpaulo/miniconda3/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import mamba_ssm\n",
    "from nnsight import LanguageModel, util\n",
    "from nnsight.tracing.Proxy import Proxy\n",
    "from nnsight.models.Mamba import MambaInterp\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "\n",
    "from typing import List, Callable, Union\n",
    "\n",
    "device = t.device(\"cuda:2\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", padding_side=\"left\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "mamba_model = MambaInterp(\"state-spaces/mamba-2.8b\", device=device, tokenizer=tokenizer)\n",
    "sampling_kwargs = {\n",
    "    \"top_p\": 0.2,\n",
    "    \"top_k\": 0,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "} # in mamba_ssm/utils/generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23c66f72fe74221a9317c5761a04608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset.from_json(\"../tuned-lens/val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Catalonia election: Puigdemont calls for talks with Spain Published duration 22 December 2017 Related Topics Catalonia independence protests\\n\\nimage copyright Reuters image caption \"Now is the time for dialogue,\" said Carles Puigdemont\\n\\nCatalonia\\'s ousted leader, Carles Puigdemont, has called for new talks with Spain after separatist parties won a slim majority in a regional election.\\n\\nHe said he wanted the negotiations in Brussels, where he is living in self-imposed exile, or another EU country.\\n\\nSpain\\'s Prime Minister Mariano Rajoy later appeared to reject the idea.\\n\\nHe said he would hold talks with the head of the new Catalan government but that leader would have to take up their post in Catalonia itself.\\n\\nHe avoided naming Mr Puigdemont, adding that the winner of Thursday\\'s election was Inés Arrimadas, the leader of the Citizens party, which wants Catalonia to remain a semi-autonomous part of Spain.\\n\\nThe Citizens party is now the region\\'s biggest party. although pro-independence parties are best placed to form a government.\\n\\n\"Catalonia wants to be an independent state,\" said Mr Puigdemont, speaking in Belgium on Friday. \"This is the wish of the Catalan people. I think the plan of [Spanish Prime Minister] Mariano Rajoy is not working, so we have to find new ways to tackle this crisis.\"\\n\\nMr Rajoy\\'s conservative Popular Party (PP) recorded its worst ever result in Thursday\\'s vote.\\n\\nHe had hoped that the poll would restore stability and said the Spanish government was \"willing to talk in a realistic way and inside the law\" with a future Catalan government.\\n\\n\"I offer Catalonia this because we care about the people\" he said.\\n\\nThe Spanish government imposed direct rule on Catalonia and called the election after declaring an October independence referendum illegal.\\n\\nMr Puigdemont has also called on the prime minister to repatriate all the police sent to Catalonia before the referendum.\\n\\nWhat were the results?\\n\\nWith nearly all votes counted, the pro-independence parties Together for Catalonia (JxCat), Republican Left of Catalonia (ERC) and Popular Unity (CUP) were on course to win a total of 70 seats in total, giving them a majority in the new parliament.\\n\\nCitizens (Cs) had 25.3% of the vote, winning 37 seats in the 135-seat chamber.\\n\\nIts leader told the BBC her party had been \"victorious\". Ms Inés Arrimadas said forming a coalition would be \"difficult - but we will try\".\\n\\nThe PP, meanwhile, won only three seats, down from 11 in the previous assembly.\\n\\nTurnout was more than 80%, a record for a Catalan regional election.\\n\\nAnalysis: What the papers say\\n\\nBy BBC Monitoring\\n\\nLeading Spanish newspapers say that the result has strengthened the government\\'s position.\\n\\n\"Nationalism can no longer claim that it exclusively represents Catalonia,\" says Madrid-based La Razón. ABC newspaper thinks Madrid should now settle the Catalan crisis. \"If Spain wants to win this fight in the long term and prevent Catalonia from leaving one day, it should draft a serious plan for strengthening the state.\"\\n\\nThe result seems to have split Catalan papers between those who want the independence project to continue, and those who accept the realpolitik of the election result.\\n\\n\"The independence movement has humiliated the Spanish prime minister,\" El Nacional says. \"The decisions that affect Catalonia are not made in Madrid.\"\\n\\nBut Barcelona\\'s El Periódico says the result means a \"divided Catalonia\". \"The election that Mariano Rajoy called has shown that Catalonia is firmly divided in two blocs and there is hardly any space for intermediaries.\"\\n\\nLa Vanguardia writes: \"Major forces supporting independence should look back, confess to mistakes and avoid making them again,\"\\n\\nWhy did the election take place?\\n\\nSeparatists who dominated the previous Catalan parliament declared independence on 27 October after a referendum that was declared illegal by Spain.\\n\\nIn an attempt to stop that referendum, Spanish police stormed some polling stations. However many voters defied the Spanish courts and riot police to cast their ballots.\\n\\nThe move led to violent clashes with hundreds of people reported injured.\\n\\nAccording to referendum organisers, 90% of voters were in favour of independence, but fewer than half the region\\'s electorate took part.\\n\\nimage copyright Getty Images image caption Inés Arrimadas said she would try to form a coalition\\n\\nHowever, Mr Puigdemont decided it was enough to declare independence from Spain.\\n\\nMr Rajoy then sacked the Catalan government, imposed direct rule and called the 21 December election.\\n\\nProsecutors accused 13 Catalan separatist politicians of rebellion and sedition, including Mr Puigdemont and four others who fled to Belgium.\\n\\nAmong the accused, two pro-independence politicians are in Spanish prisons, and six are being monitored while on bail.\\n\\nWhat has been the reaction?\\n\\nThe European Commission said that its stance towards Catalonia remained the same, despite Thursday\\'s election result.\\n\\nThe executive arm of the EU has previously stated that events in Catalonia are an internal issue for Spain.\\n\\n\"Our position on the question of Catalonia is well known and has been regularly restated, at all levels. It will not change,\" commission spokesman Alexander Winterstein told AFP news agency.\\n\\n\"In relation to a regional election, we have no comment to make,\" he added.\\n\\nThe Spanish government has not yet commented on the results.\\n\\nWhat happens now?\\n\\nAnalysts say the success of separatist parties means that the ball is now back in the Spanish government\\'s court.\\n\\nAntonio Barroso, of the London-based research firm Teneo Intelligence, says the problem for Madrid remains \"and the secession movement is not going to go away\".\\n\\nCorrespondents say it is not yet clear whether Mr Puigdemont will be renamed president, and if so, if he will return from Belgium. As things stand, he faces arrest, should he enter Spain.\\n\\nimage copyright Getty Images image caption Independence supporters celebrated in Barcelona\\n\\nWhy do many Catalans want independence?\\n\\nCatalonia is one of Spain\\'s wealthiest and most productive regions and has a distinct history dating back almost 1,000 years.\\n\\nBefore the Spanish Civil War it enjoyed broad autonomy but that was suppressed under General Francisco Franco\\'s dictatorship from 1939-75.\\n\\nWhen Franco died, the region was granted autonomy again under the 1978 constitution, and the region prospered along with the rest of the new, democratic Spain.\\n\\nA 2006 statute granted even greater powers, boosting Catalonia\\'s financial clout and describing it as a \"nation\", but Spain\\'s Constitutional Court reversed much of this in 2010.\\n\\nRecession and cuts in public spending fuelled local resentment, which coalesced in a powerful secessionist movement.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n",
      "torch.Size([5120, 16])\n"
     ]
    }
   ],
   "source": [
    "with mamba_model.generate(max_length=10,**sampling_kwargs,validate=False) as generator:\n",
    "            \n",
    "    with generator.invoke(\"Where is the Eiffel tower located at?\",scan=True) as invoker:\n",
    "        \n",
    "        ssm_input = mamba_model.backbone.layers[0].mixer.ssm.input.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(tokens_str,logits):\n",
    "    logits = logits[:-1,: ].contiguous()\n",
    "    tokens = tokenizer(tokens_str, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "    labels = tokens[:, 1:].contiguous().T\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5119, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "with mamba_model.invoke(dataset[\"text\"][0][:800],scan=True) as invoker:\n",
    "\n",
    "    # for layer in mamba_model.backbone.layers:\n",
    "    #     A_bar = layer.mixer.ssm.discA.output.save()\n",
    "    #     dB = layer.mixer.ssm.discB.output.save()\n",
    "    #     dA = layer.mixer.ssm.dA.output.save()\n",
    "    #     zohDeltaB=dB*1/dA*(A_bar-1) \n",
    "    #     layer.mixer.ssm.discB.output=zohDeltaB\n",
    "\n",
    "    pass\n",
    "output=invoker.output\n",
    "loss = compute_loss(dataset[\"text\"][0][:800],output.logits[0])  \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1857, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "with mamba_model.invoke(dataset[\"text\"][0][:800],scan=True) as invoker:\n",
    "\n",
    "    for layer in mamba_model.backbone.layers:\n",
    "        A_bar = layer.mixer.ssm.discA.output\n",
    "        dB = layer.mixer.ssm.discB.output\n",
    "        dA = layer.mixer.ssm.dA.output\n",
    "        zohDeltaB=dB*1/dA*(A_bar-1) \n",
    "        layer.mixer.ssm.discB.output=zohDeltaB\n",
    "\n",
    "output=invoker.output\n",
    "loss = compute_loss(dataset[\"text\"][0][:800],output.logits[0])  \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.9072, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "with mamba_model.invoke(dataset[\"text\"][0][:800],scan=True) as invoker:\n",
    "\n",
    "    for layer in mamba_model.backbone.layers:\n",
    "        A_bar = layer.mixer.ssm.discA.output\n",
    "        dB = layer.mixer.ssm.discB.output\n",
    "        dA = layer.mixer.ssm.dA.output\n",
    "        zohDeltaB=dB*1/dA*(A_bar-1) \n",
    "        layer.mixer.ssm.discB.output=t.zeros_like(zohDeltaB)\n",
    "        layer.mixer.ssm.discA.output=t.zeros_like(A_bar)\n",
    "\n",
    "output=invoker.output\n",
    "loss = compute_loss(dataset[\"text\"][0][:800],output.logits[0])  \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaLMHeadModel(\n",
       "  (backbone): MixerModel(\n",
       "    (embedding): Embedding(50280, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-63): 64 x Block(\n",
       "        (mixer): MambaModuleInterp(\n",
       "          (in_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "          (conv1d): Conv1d(5120, 5120, kernel_size=(4,), stride=(1,), padding=(3,), groups=5120)\n",
       "          (act): SiLU()\n",
       "          (x_proj): Linear(in_features=5120, out_features=192, bias=False)\n",
       "          (dt_proj): Linear(in_features=160, out_features=5120, bias=True)\n",
       "          (out_proj): Linear(in_features=5120, out_features=2560, bias=False)\n",
       "          (dt): WrapperModule()\n",
       "          (B): WrapperModule()\n",
       "          (C): WrapperModule()\n",
       "          (ssm): SSM(\n",
       "            (discA): DiscA()\n",
       "            (discB): DiscB()\n",
       "            (dA): DA()\n",
       "            (hx): Hx(\n",
       "              (bx): Bx()\n",
       "              (ah): Ah()\n",
       "            )\n",
       "            (yh): Yh()\n",
       "          )\n",
       "          (delta_softplus): Softplus(beta=1, threshold=20)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mamba_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22.7835, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "with mamba_model.invoke(dataset[\"text\"][0][:50],scan=True) as invoker:\n",
    "\n",
    "    for layer in mamba_model.backbone.layers:\n",
    "        out = layer.mixer.conv1d.output[0][:,:]\n",
    "        layer.mixer.conv1d.output[0][...,:] = t.concat([layer.mixer.conv1d.input[0][0][0][:,:],t.zeros_like(out)[:,:3]],dim=1)  #A_bar = layer.mixer.ssm.discA.output\n",
    "        \n",
    "output=invoker.output\n",
    "loss = compute_loss(dataset[\"text\"][0][:50],output.logits[0])  \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42.1457, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "with mamba_model.invoke(dataset[\"text\"][0][:50],scan=True) as invoker:\n",
    "\n",
    "    out = mamba_model.backbone.layers[0].mixer.conv1d.output[0][:,:]\n",
    "    mamba_model.backbone.layers[0].mixer.conv1d.output[0][...,:] = t.concat([mamba_model.backbone.layers[0].mixer.conv1d.input[0][0][0][:,:],t.zeros_like(out)[:,:3]],dim=1)  #A_bar = layer.mixer.ssm.discA.output\n",
    "        \n",
    "output=invoker.output\n",
    "loss = compute_loss(dataset[\"text\"][0][:50],output.logits[0])  \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(176.0256, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "with mamba_model.invoke(dataset[\"text\"][0][:50],scan=True) as invoker:\n",
    "\n",
    "    for layer in mamba_model.backbone.layers:\n",
    "        out = layer.mixer.conv1d.output[0].save()\n",
    "        layer.mixer.conv1d.output[0]= t.zeros_like(out) \n",
    "        \n",
    "output=invoker.output\n",
    "loss = compute_loss(dataset[\"text\"][0][:50],output.logits[0])  \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 50280])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoker.output.logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2534, -0.4671, -0.5139,  ...,  0.6724, -0.2481,  0.2129],\n",
       "       device='cuda:2')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_input.value[0][0][0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0650,  0.0308,  0.0203,  ..., -0.0028,  0.2437, -0.0757],\n",
       "       device='cuda:2')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_output.value[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
